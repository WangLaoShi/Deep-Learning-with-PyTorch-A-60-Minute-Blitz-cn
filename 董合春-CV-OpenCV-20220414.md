# Pytorch 笔记——董合春
## 一、PyTorch简介
### 1.1 PyTorch 和 NumPy
PyTorch是NumPy的替代品，两者都是python的科学计算包，可以高效地解决python中特定的问题，因此二者在许多代码方面互通。在NumPy被开发的年代GPU还未被开发，所以PyTprch作为NumPy的替代品可以利用GPU的性能进行计算，从而成为一个高灵活性、速度快的深度学习平台。
## 二、PyTorch基础
### 2.1 张量的概念
张量类似于NumPy的ndarray,在GPU上用来加速运算。
**导入方法：**
`from __future__ import print_function`
`import torch`
**各种创建代码：**
- 未初始化的5 * 3矩阵`x = torch.empty(5, 3)`
- 随机初始化的矩阵`x = torch.rand(5, 3)`
- 填满零且数据类型为long`x = torch.zeros(5, 3, dtype=torch.long)`
- 直接从数据构造张量`x = torch.tensor([5.5, 3])`

**一些高级一点点的代码：**
根据已有的tensor建立新的tensor。如果用户不提供新的值，这些方法会重用输入的张量的属性
- `x = x.new_ones(5, 3, dtype=torch.double)`
- `x = torch.randn_like(x, dtype=torch.float)`

### 2.2 张量的操作
1. 获取它的形状：`print(x.size())`
2. 运算（加法）：
	- 形式一：`print(x + y)`
	- 形式二：`print(torch.add(x, y))`
	- 形式三：```result = torch.empty(5, 3)
	torch.add(x, y, out=result)
	print(result)```这种方法是在加法结束后替换掉原来result的值。
	- 形式四：`y.add_(x)`这种方法称为‘’in-place‘’法。
	*需要注意的是：任何一个in-place改变张量的操作后面都固定有一个`_`作为标识。例如`x.copy_(y)`、`x.t_()`将更改x原本的值。*
3. 索引：`print(x[:, 1])`输出所有行第二列的值。
4. 改变形状：
	- `x = torch.randn(4, 4)`创建一个4* 4的张量。
	- `y = x.view(16)` 以16行的格式输出。
	- `z = x.view(-1, 8)`-1看作n,将原来的数组按照n行8列输出。当然在这里就是2行8列。
### 2.3 扩展
超过100中tensor的运算操作，包括转置，索引，切片，数学运算， 线性代数，随机数等，具体访问：
>https://pytorch.org/docs/stable/torch.html

## 三、NumPy桥
### 3.1 简介
上文提到PyTorch和NumPy在许多代码方面互通，因此将一个Torch张量转换为一个NumPy数组是轻而易举的事情，反之亦然。
Torch张量和NumPy数组将共享它们的底层内存位置，因此更改一个将更改另一个。
### 3.2 相关代码
1. 改变tensor来改变numpy：
`a = torch.ones(5)`
`b = a.numpy()`
`a.add_(1)`此时a和b同时加一
2. 改变numpy来改变tensor：
```
import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)
np.add(a, 1, out=a)
```
输出的a和b同时加一
## 四、CUDA的张量
### 简介：
利用`torch.device`将tensor移入和移出GPU，从而实现在GPU上进行的运算。
### 代码：
```
if torch.cuda.is_available():
    device = torch.device("cuda")          # a CUDA device object
    y = torch.ones_like(x, device=device)  # 直接在GPU上创建tensor
    x = x.to(device)                       # 或者使用`.to("cuda")`方法
    z = x + y
    print(z)
    print(z.to("cpu", torch.double))       # `.to`也能在移动时改变dtype
```
