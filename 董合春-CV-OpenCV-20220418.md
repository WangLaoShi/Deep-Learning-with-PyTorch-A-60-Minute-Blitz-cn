# CNN——董合春
## 一、CNN 引入
### 1.1 神经网络
人工神经网络（Artiﬁcial Neural Network，ANN），也简称神经网络，是众多机器学习算法中比较接近生物神经网络特性的数学模型。
它包含：
- CNN：卷积神经网络
- RNN：循环神经网络
- MNN：深度神经网络
### 1.2 卷积神经网络
原因：在全连接神经网络中相邻两层之间的神经元都有边相连，但当神经元增多时会导致网络参数异常庞大从而降低计算速度。

公式：y=x*w+b;

在卷积神经网络中，卷积层的神经元仅与前一层的***部分神经元***相连，且同一层中某些神经元之间的链接的***权重w和偏移b***时共享（相同）的，大大减少需要训练参数的数量。

## 二、层次结构
![[Pasted image 20220418204719.png]]
### 2.1 输入层
对于28×28的灰度图，输入层为二维神经网络；
对于28×28的彩色图，输入层为三维神经网络。其中RGB每个通道都是一个28×28的矩阵。
### 2.2 卷积层
两个概念：
local receptive fields（感受视野）
shared weights（共享权值）
如下图：
- 感受视野代表黄色圈层的部分，即我们自己选定的方格，
- 设置移动步长为1，即每次扫描完毕后右移一格，整行扫描完毕后下移一格。当移动出界时需要进行边界扩充，边界扩充设置为零或其他值。
- 共享权值即圈定的方格内的 ”×加数字“，感受视野内的值经过共享权值的计算后相加，即为卷积后的值
![[Pasted image 20220418205550.png]]
我们称扫描结束后生成的下一层神经元矩阵为（feature map，特征映射图），由于在同一个feature map上的神经元使用的卷积核是相同的，所以称为shared weights（共享权值）。
### 2.3 激励层
由于卷积是线性运算，所以激励层是对卷积层的输出的一个非线性映射。在之前的概念中成为激励函数，一般使用ReLu函数：`f(x)=max(x,0)`
### 2.4 池化层
有一说一，经过卷积层之后若视野和步长较小，得到的feature map还是很大，所以要通过池化层对每个feature map进行降维操作，但输出的深度不变。

池化层有filter（池化视野）对feature map进行扫描，有两种计算方式对filter进行计算：
1. Max pooling：取filter中最大值
2. Average pooling：去filter中平均值
![[Pasted image 20220418212239.png]]
### 2.5 全连接层和输出层
全连接层对信息进行重新拟合，减少特征信息的丢失；
输出层主要准备输出最后的目标结果。

### 2.6* 归一化层
Batch Normalization（批量归一化）可以在网络层之间实现数据预处理，防止梯度弥散，加速网络训练。
简单来说，就是把上一层输出的数据变得更接近。

一般调用一个函数来解决：`.normalization(fc)`
### 2.7* 切分层和融合层
在某些应用中，需要对图片进行切割，独立对某一部分进行单独学习。
而融合层可以对切分层进行融合，也可以对不同大小卷积核学习到的特征进行融合。
